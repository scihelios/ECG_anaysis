import numpy as np
import os
import json
from sklearn.model_selection import train_test_split

# LMS Filter Class
class LMSFilter:
    def __init__(self, filter_size, learning_rate):
        self.weights = np.zeros(filter_size)
        self.learning_rate = learning_rate

    def predict(self, input_vector):
        return np.dot(self.weights, input_vector)

    def update(self, input_vector, desired_output):
        prediction = self.predict(input_vector)
        error = desired_output - prediction
        self.weights += self.learning_rate * error * input_vector
        return error

def load_data(folder_path, window_size):
    all_sequences = []
    all_targets = []
    
    for file_name in os.listdir(folder_path):
        if file_name.startswith('signal') and file_name.endswith('.json'):
            print(file_name)
            with open(os.path.join(folder_path, file_name), 'r') as file:
                data = json.load(file)
                long_sequence = data["signal"]
                for i in range(len(long_sequence) - window_size):
                    window = long_sequence[i:i + window_size]
                    next_point = long_sequence[i + window_size]
                    all_sequences.append(window)
                    all_targets.append(next_point)

    return np.array(all_sequences), np.array(all_targets)

window_size = 400
inputs, targets = load_data('C:/Users/ahmed mansour/Desktop/scolarite X/2A/Psc/ECG_anaysis/for_lsm', window_size)

# Split the data into training and validation sets
train_inputs, val_inputs, train_targets, val_targets = train_test_split(
    inputs, targets, test_size=0.2, random_state=42)


# Initialize the LMS Filter
window_size = 400  # Size of each sequence
lms_filter = LMSFilter(window_size, learning_rate=0.001)

# Training the LMS Filter
for input_seq, target in zip(train_inputs, train_targets):
    lms_filter.update(np.array(input_seq), target)

def iterative_forecast(initial_data, model, steps):
    """
    Predict future points iteratively using the LMS model.

    :param initial_data: List of initial 400 points.
    :param model: Trained LMS filter.
    :param steps: Number of future points to predict.
    :return: List containing the initial data followed by the predicted points.
    """
    data = initial_data.copy()
    
    for _ in range(steps):
        print(_)
        # Use the latest 400 points for prediction
        current_input = np.array(data[-400:])
        next_point = model.predict(current_input)
        data.append(next_point)
    return data
import matplotlib.pyplot as plt
# Assuming `lms_filter` is your trained LMS filter
# And `initial_points` is your list of initial 400 points
initial_points = [ -0.007194244604316547, 0.0, 0.0, 0.007194244604316547, 0.007194244604316547, 0.007194244604316547, 0.007194244604316547, 0.0, 0.0, -0.007194244604316547, -0.007194244604316547, -0.014388489208633094, -0.02158273381294964, -0.02158273381294964, -0.02877697841726619, -0.04316546762589928, -0.050359712230215826, -0.05755395683453238, -0.07194244604316546, -0.07913669064748201, -0.07913669064748201, -0.08633093525179857, -0.07913669064748201, -0.07913669064748201, -0.07194244604316546, -0.05755395683453238, -0.05755395683453238, -0.050359712230215826, -0.050359712230215826, -0.050359712230215826, -0.050359712230215826, -0.05755395683453238, -0.050359712230215826, -0.050359712230215826, -0.04316546762589928, -0.03597122302158273, -0.02158273381294964, -0.014388489208633094, -0.014388489208633094, -0.014388489208633094, -0.02158273381294964, -0.03597122302158273, -0.04316546762589928, -0.05755395683453238, -0.06474820143884892, -0.06474820143884892, -0.05755395683453238, -0.04316546762589928, -0.02877697841726619, -0.02158273381294964, -0.007194244604316547, 0.0, -0.007194244604316547, -0.007194244604316547, -0.02158273381294964, -0.03597122302158273, -0.050359712230215826, -0.05755395683453238, -0.06474820143884892, -0.06474820143884892, -0.06474820143884892, -0.05755395683453238, -0.050359712230215826, -0.04316546762589928, -0.04316546762589928, -0.04316546762589928, -0.050359712230215826, -0.05755395683453238, -0.05755395683453238, -0.06474820143884892, -0.06474820143884892, -0.05755395683453238, -0.050359712230215826, -0.04316546762589928, -0.03597122302158273, -0.02877697841726619, -0.02877697841726619, -0.02877697841726619, -0.03597122302158273, -0.04316546762589928, -0.050359712230215826, -0.050359712230215826, -0.05755395683453238, -0.05755395683453238, -0.050359712230215826, -0.050359712230215826, -0.04316546762589928, -0.04316546762589928, -0.04316546762589928, -0.04316546762589928, -0.04316546762589928, -0.04316546762589928, -0.04316546762589928, -0.03597122302158273, -0.03597122302158273, -0.02877697841726619, -0.02158273381294964, -0.007194244604316547, 0.0, 0.007194244604316547, 0.02158273381294964, 0.02877697841726619, 0.04316546762589928, 0.050359712230215826, 0.06474820143884892, 0.07194244604316546, 0.07913669064748201, 0.07913669064748201, 0.07194244604316546, 0.06474820143884892, 0.05755395683453238, 0.050359712230215826, 0.04316546762589928, 0.03597122302158273, 0.02877697841726619, 0.02158273381294964, 0.014388489208633094, 0.0, -0.014388489208633094, -0.03597122302158273, -0.050359712230215826, -0.06474820143884892, -0.07194244604316546, -0.07194244604316546, -0.06474820143884892, -0.050359712230215826, -0.03597122302158273, -0.02877697841726619, -0.02158273381294964, -0.02877697841726619, -0.04316546762589928, -0.06474820143884892, -0.09352517985611511, -0.11510791366906475, -0.1366906474820144, -0.14388489208633093, -0.1366906474820144, -0.1223021582733813, -0.1079136690647482, -0.07913669064748201, -0.06474820143884892, -0.050359712230215826, -0.050359712230215826, -0.050359712230215826, -0.06474820143884892, -0.08633093525179857, -0.10071942446043165, -0.11510791366906475, -0.1223021582733813, -0.1223021582733813, -0.1223021582733813, -0.11510791366906475, -0.1079136690647482, -0.10071942446043165, -0.10071942446043165, -0.1079136690647482, -0.1079136690647482, -0.1223021582733813, -0.12949640287769784, -0.14388489208633093, -0.15827338129496402, -0.16546762589928057, -0.17266187050359713, -0.17266187050359713, -0.15827338129496402, -0.12949640287769784, -0.07913669064748201, -0.014388489208633094, 0.08633093525179857, 0.19424460431654678, 0.3237410071942446, 0.460431654676259, 0.5827338129496403, 0.6834532374100719, 0.7553956834532374, 0.7769784172661871, 0.7553956834532374, 0.6906474820143885, 0.5755395683453237, 0.43884892086330934, 0.2733812949640288, 0.11510791366906475, -0.04316546762589928, -0.17266187050359713, -0.2733812949640288, -0.3381294964028777, -0.3669064748201439, -0.3669064748201439, -0.34532374100719426, -0.302158273381295, -0.2589928057553957, -0.2158273381294964, -0.17266187050359713, -0.14388489208633093, -0.1223021582733813, -0.1079136690647482, -0.10071942446043165, -0.10071942446043165, -0.1079136690647482, -0.1079136690647482, -0.1079136690647482, -0.1079136690647482, -0.10071942446043165, -0.09352517985611511, -0.09352517985611511, -0.08633093525179857, -0.07913669064748201, -0.07194244604316546, -0.07194244604316546, -0.07194244604316546, -0.07194244604316546, -0.06474820143884892, -0.06474820143884892, -0.05755395683453238, -0.05755395683453238, -0.050359712230215826, -0.04316546762589928, -0.03597122302158273, -0.03597122302158273, -0.02877697841726619, -0.02877697841726619, -0.02877697841726619, -0.02877697841726619, -0.02158273381294964, -0.02158273381294964, -0.02158273381294964, -0.02158273381294964, -0.02877697841726619, -0.02877697841726619, -0.03597122302158273, -0.04316546762589928, -0.04316546762589928, -0.04316546762589928, -0.03597122302158273, -0.02877697841726619, -0.02158273381294964, -0.007194244604316547, 0.0, 0.007194244604316547, 0.007194244604316547, 0.007194244604316547, 0.0, -0.007194244604316547, -0.014388489208633094, -0.014388489208633094, -0.007194244604316547, 0.007194244604316547, 0.02158273381294964, 0.03597122302158273, 0.05755395683453238, 0.06474820143884892, 0.06474820143884892, 0.06474820143884892, 0.05755395683453238, 0.050359712230215826, 0.050359712230215826, 0.05755395683453238, 0.07194244604316546, 0.10071942446043165, 0.12949640287769784, 0.15827338129496402, 0.17985611510791366, 0.18705035971223022, 0.19424460431654678, 0.18705035971223022, 0.17266187050359713, 0.16546762589928057, 0.16546762589928057, 0.17266187050359713, 0.19424460431654678, 0.2302158273381295, 0.26618705035971224, 0.302158273381295, 0.33093525179856115, 0.35251798561151076, 0.3597122302158273, 0.3597122302158273, 0.35251798561151076, 0.34532374100719426, 0.3381294964028777, 0.3381294964028777, 0.34532374100719426, 0.3597122302158273, 0.381294964028777, 0.41007194244604317, 0.4316546762589928, 0.4460431654676259, 0.460431654676259, 0.4676258992805755, 0.4748201438848921, 0.4748201438848921, 0.4676258992805755, 0.460431654676259, 0.4460431654676259, 0.4316546762589928, 0.41007194244604317, 0.381294964028777, 0.3597122302158273, 0.33093525179856115, 0.302158273381295, 0.2805755395683453, 0.26618705035971224, 0.2517985611510791, 0.2446043165467626, 0.23741007194244604, 0.22302158273381295, 0.2158273381294964, 0.19424460431654678, 0.17266187050359713, 0.14388489208633093, 0.1079136690647482, 0.07913669064748201, 0.050359712230215826, 0.02877697841726619, 0.007194244604316547, 0.0, -0.007194244604316547, -0.014388489208633094, -0.02158273381294964, -0.02158273381294964, -0.02877697841726619, -0.02877697841726619, -0.03597122302158273, -0.03597122302158273, -0.04316546762589928, -0.04316546762589928, -0.04316546762589928, -0.04316546762589928, -0.04316546762589928, -0.04316546762589928, -0.04316546762589928, -0.04316546762589928, -0.04316546762589928, -0.050359712230215826, -0.050359712230215826, -0.05755395683453238, -0.06474820143884892, -0.07194244604316546, -0.07913669064748201, -0.08633093525179857, -0.09352517985611511, -0.10071942446043165, -0.1079136690647482, -0.11510791366906475, -0.1223021582733813, -0.12949640287769784, -0.12949640287769784, -0.12949640287769784, -0.1366906474820144, -0.1366906474820144, -0.1366906474820144, -0.14388489208633093, -0.14388489208633093, -0.1510791366906475, -0.1510791366906475, -0.15827338129496402, -0.15827338129496402, -0.1510791366906475, -0.14388489208633093, -0.1366906474820144, -0.1223021582733813, -0.1079136690647482, -0.09352517985611511, -0.08633093525179857, -0.08633093525179857, -0.08633093525179857, -0.09352517985611511, -0.10071942446043165, -0.11510791366906475, -0.1223021582733813, -0.1223021582733813, -0.1223021582733813, -0.11510791366906475, -0.10071942446043165, -0.08633093525179857, -0.07913669064748201, -0.06474820143884892, -0.05755395683453238, -0.05755395683453238, -0.06474820143884892, -0.07194244604316546, -0.07913669064748201, -0.08633093525179857, -0.09352517985611511, -0.09352517985611511, -0.09352517985611511, -0.09352517985611511, -0.09352517985611511, -0.09352517985611511, -0.08633093525179857, -0.08633093525179857, -0.08633093525179857, -0.08633093525179857, -0.08633093525179857, -0.07913669064748201, -0.07194244604316546, -0.06474820143884892, -0.05755395683453238, -0.05755395683453238, -0.050359712230215826] # Replace with your actual initial data

predicted_points = iterative_forecast(initial_points, lms_filter, steps=500)

# Plotting
plt.figure(figsize=(12, 6))
plt.plot(range(400), initial_points, label='Initial Data')
plt.plot(range(400, 900), predicted_points[400:], label='Predicted Data')
plt.title("ECG Signal Forecasting")
plt.xlabel("Time Steps")
plt.ylabel("ECG Signal Value")
plt.legend()
plt.show()

