
import torch
import torch.nn as nn
import torch.optim as optim
import os
import json
import numpy as np
import numpy as np
from scipy.signal import find_peaks
import matplotlib.pyplot as plt
import json
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.optim as optim
import os
import json
import numpy as np
from sklearn.preprocessing import StandardScaler



def gaussian(x, A, mu, sigma):
    return A * np.exp(-(x - mu)**2 / (2 * sigma**2)) / (np.sqrt(2*3.14)*sigma)

# Define the combined model with 5 Gaussian functions
def combined_gaussian(x, *params):
    return sum([gaussian(x, *params[i:i+3]) for i in range(0, len(params), 3)])

from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler

# Check for GPU availability
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



class EnhancedLinearGaussianPredictor(nn.Module):
    def __init__(self):
        super(EnhancedLinearGaussianPredictor, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(1500, 100),
            nn.Linear(100, 1500),
              # First linear layer, expanding to 128 units
  # Second linear layer, reducing to 64 units
        )

    def forward(self, x):
        return self.network(x)

# Early Stopping Class
class EarlyStopping:
    def __init__(self, patience=10, delta=0):
        self.patience = patience
        self.delta = delta
        self.best_loss = None
        self.counter = 0
        self.early_stop = False

    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss + self.delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0

# Training Function
def train_model(model, train_loader, val_inputs, val_targets, criterion, optimizer, epochs=500, patience=100):
    early_stopping = EarlyStopping(patience=patience)
    val_inputs, val_targets = val_inputs, val_targets

    for epoch in range(epochs):
        model.train()
        for inputs, targets in train_loader:
            inputs, targets = inputs, targets

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

        model.eval()
        with torch.no_grad():
            val_outputs = model(val_inputs)
            val_loss = criterion(val_outputs, val_targets)

        print(f'Epoch {epoch+1}, Validation Loss: {val_loss.item()}')

        early_stopping(val_loss.item())
        if early_stopping.early_stop:
            print("Early stopping")
            break

    # Save the model
    torch.save(model.state_dict(), "enhanced_model.pth")
    print("Model saved.")


# Define the load_data function
def load_data(base_path):
    s=0
    all_sequences = []
    all_targets = []
    for folder_name in os.listdir(base_path):
        folder_path = base_path+'/'+folder_name
        
        # Check if it's a directory
        if os.path.isdir(folder_path) and s<100:
            s+=1
            print(folder_path)
            for i in range(len(os.listdir(folder_path))-2):
                test1= []
                test2 =[]
                with open(os.path.join(folder_path, os.listdir(folder_path)[i]), 'r') as file:
                    data = json.load(file)
                    test1 = data["signal"]+[0 for i in range(1499-len(data["signal"]))]
                    time = data["temps_prochain_signal"]*1000
                    test1.append(time)
                with open(os.path.join(folder_path, os.listdir(folder_path)[i+1]), 'r') as file:
                    data = json.load(file)
                    test2 = data["signal"]+[0 for i in range(1499-len(data["signal"]))]
                    time = data["temps_prochain_signal"]*1000
                    test2.append(time)
                    all_sequences.append(test1)
                    all_targets.append(test2)

    return np.array(all_sequences), np.array(all_targets)



# Assume the dataset is loaded using the load_data function
folder_path = "C:/Users/ahmed mansour/Desktop/scolarite X/2A/Psc/ptb-diagnostic-ecg-database-1.0.0/cleaned_data"
train_inputs, train_targets = load_data(folder_path)
val_inputs, val_targets = train_inputs, train_targets  # You can use the same function for validation data
model_save_path = "model.pth"

# Convert to torch tensors
train_inputs = torch.tensor(train_inputs, dtype=torch.float32)
train_targets = torch.tensor(train_targets, dtype=torch.float32)
val_inputs = torch.tensor(val_inputs, dtype=torch.float32)
val_targets = torch.tensor(val_targets, dtype=torch.float32)

# DataLoader
train_dataset = TensorDataset(train_inputs, train_targets)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# Model
model = EnhancedLinearGaussianPredictor()

# Loss and Optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.0005)

# Train the model
train_model(model, train_loader, val_inputs, val_targets, criterion, optimizer)

'''
# Evaluation
model.eval() # Set the model to evaluation mode
with torch.no_grad():
    total_loss = 0
    for i in range(len(val_inputs)):
        inputs = torch.tensor(val_inputs[i], dtype=torch.float32)
        targets = torch.tensor(val_targets[i], dtype=torch.float32)
        predictions = model(inputs)
        loss = criterion(predictions, targets)
        total_loss += loss.item()
    avg_loss = total_loss / len(val_inputs)
    print(f'Validation Loss: {avg_loss}')'''

def iterative_forecast(initial_data, model, steps):
    """
    Predict future sequences iteratively using the LinearGaussianPredictor model.

    :param initial_data: List of initial data points, should be a multiple of 13.
    :param model: Trained LinearGaussianPredictor model.
    :param steps: Number of sequences (each of 13 points) to predict.
    :return: List containing the initial data followed by the predicted sequences.
    """
    data = []
    model.eval()  
    current_input = torch.tensor(initial_data, dtype=torch.float32)
    current_input = current_input.unsqueeze(0)  
    with torch.no_grad():
        for _ in range(1,steps):
            prediction = model(current_input)[0].tolist()
            plt.plot([_*1500 + i for i in range(1499)] ,prediction[0:1499], color ='red')
            current_input = prediction
            current_input = torch.tensor(current_input, dtype=torch.float32)
            current_input = current_input.unsqueeze(0) 



    plt.title("ECG Signal Forecasting")
    plt.xlabel("Time Steps")
    plt.ylabel("ECG Signal Value")
    plt.show()
    
    return data

initial_points = [0.0, -0.0009529425445821971, -0.002047886090209117, -0.0031794345579650163, -0.004455412786602526, -0.005885582583192189, -0.00721039064144427, -0.008399716709187932, -0.00941163064297851, -0.010177084454421862, -0.010744081793686439, -0.010944728562161887, -0.010666063769541073, -0.01005247034663473, -0.009207814779343793, -0.008290153164115799, -0.007434250845063967, -0.006666134734040708, -0.0061369162829577615, -0.005887316652817637, -0.0059405813970170585, -0.006266175307143158, -0.006738284171181803, -0.007417701080871837, -0.00822586295673586, -0.008957644921913257, -0.009547445005806093, -0.01006866836171813, -0.010500349131134623, -0.010582703575295062, -0.010288701010515712, -0.009685925435828387, -0.008810723992043246, -0.007805523430064245, -0.006610191999477739, -0.005239369948290385, -0.003995327318884468, -0.0030769787503551202, -0.0025065371882417044, -0.002323614719481311, -0.002549890839733144, -0.003120265792064565, -0.003936994792693633, -0.004841101163371137, -0.005742956961588914, -0.006566825033375691, -0.007243694398519159, -0.007817934454321137, -0.008169038181694956, -0.008247253497468911, -0.008180650502531694, -0.007941891249752949, -0.0074723900272103145, -0.006828026823501783, -0.006196738600805636, -0.005618920936944246, -0.005147735030873237, -0.0048280728615570385, -0.00456253841830956, -0.004476351508999307, -0.004616974026193628, -0.0049511262197009815, -0.0054247285142896015, -0.006031604359099165, -0.006753176277401354, -0.007351893916267812, -0.007775518065909955, -0.00808242077055771, -0.008311724057778443, -0.008506028852732968, -0.008671004267816614, -0.00879889118773622, -0.008863124331091795, -0.008895764383492101, -0.008902933093041384, -0.008926440754222583, -0.00910927979654927, -0.009453946179304593, -0.009926173038077997, -0.01046489916103391, -0.011109443128737437, -0.011887725844847848, -0.012721259365539087, -0.013645366382695813, -0.014597261263268187, -0.015542514284219381, -0.016409529105645113, -0.017198987491523543, -0.01781087208336907, -0.018094811948506095, -0.018186784590035893, -0.01808835832742237, -0.017845454605558007, -0.017552226073954017, -0.01718959202863177, -0.016780652448619097, -0.01641899594533944, -0.01612948936691758, -0.015969720000823338, -0.01602139307486961, -0.016157148496126927, -0.0162493156686582, -0.016298515017456636, -0.01636495506139277, -0.016449158861166966, -0.01638983632214031, -0.01614291147527092, -0.015777590908979015, -0.01531142061610358, -0.014785420333443825, -0.014224627810235427, -0.013600572828454786, -0.012875812091907194, -0.012177677441768179, -0.011660294157236354, -0.011420115211443819, -0.011523338948610743, -0.01186373210074618, -0.012334949487975205, -0.012921815031756713, -0.01363820641878766, -0.014391954585789266, -0.01514941165838463, -0.01586716893847687, -0.016445663726470137, -0.016913851860831303, -0.017216809958913618, -0.017370465296725066, -0.017354647030802504, -0.017223221054943068, -0.01712741765953402, -0.017006840746791572, -0.016874895933551105, -0.016726592348741524, -0.01661553369562541, -0.016582139319459066, -0.016657828091284303, -0.01691644242288876, -0.017336219878762247, -0.01800390581915074, -0.018880724274407738, -0.019935439476209824, -0.021133807515414636, -0.022352292866218378, -0.02355404607919104, -0.024582612018886174, -0.025415515640952322, -0.026078321756559057, -0.026479984901400997, -0.02653722051037286, -0.026179108986078645, -0.025532993464711665, -0.024673690201407836, -0.0236266759395884, -0.022434042647395548, -0.021183252665109377, -0.020072599815096823, -0.019119061248883688, -0.018401888337001102, -0.01792890278790649, -0.01770025214587915, -0.01782170634657621, -0.01824403753576193, -0.01891914345142521, -0.01967829702902802, -0.02034921251935099, -0.020739482498057103, -0.0207990236295908, -0.020627096912318347, -0.020192167188222063, -0.019590698460327745, -0.01882861258193425, -0.017871839271667914, -0.016818707382647017, -0.01578970526080243, -0.014971319134926558, -0.014376312973192229, -0.013873623513045944, -0.013477004604651543, -0.013226575175439991, -0.01305193832230928, -0.01287120693611788, -0.012701261984632898, -0.012561081013082764, -0.01246132872924594, -0.012374023619205815, -0.012211578071100217, -0.012000219323331457, -0.011760973537882418, -0.011443824252885709, -0.011044079861227662, -0.01064401638001009, -0.010359879233573404, -0.0101668417459718, -0.010033212968625698, -0.009948420647501987, -0.009936148747119572, -0.009963193801017128, -0.009934419873776919, -0.00979472036597393, -0.009355502807164994, -0.008648367191688522, -0.007905998934154564, -0.007219909208167152, -0.0066039878902456935, -0.00603785015958217, -0.005527509027593946, -0.005062431381416216, -0.00458808401797623, -0.004097378178183709, -0.0035928204786638334, -0.00306748612625935, -0.0025147994058835116, -0.0018441049863186015, -0.0010383131632138226, -0.0001331858178340089, 0.0007995018702211471, 0.0018094160779584196, 0.0029200503794791157, 0.004041195117713912, 0.005234106336191445, 0.006575642954445537, 0.008044773453756919, 0.009586827627698824, 0.011174023939721688, 0.012575252326668835, 0.013730739656527317, 0.014740982156028447, 0.015647372273552805, 0.016626967943958453, 0.017640498701995293, 0.018609703002491894, 0.019479041669566453, 0.020206646345874983, 0.020896523372601332, 0.021554141961171797, 0.02225545654407517, 0.023144483758755437, 0.024170329540204812, 0.025208068353403106, 0.026244256648766083, 0.027442962431262742, 0.02886717109472118, 0.03053698124160533, 0.03248708370200932, 0.034636641389136746, 0.03685908018210964, 0.0390054382010229, 0.040994577331160735, 0.04267584876451488, 0.044148775129407625, 0.04551337573189954, 0.04665619266024338, 0.04760029089508217, 0.04809738394847179, 0.0480982749019029, 0.04777684728921266, 0.04729587700099786, 0.046790511113991046, 0.046166725107626476, 0.04547806341709692, 0.044691485771547454, 0.04366463112644527, 0.042346995947242935, 0.040823221647510274, 0.039174072744554145, 0.037358095100953344, 0.03534639995951358, 0.033108389644476106, 0.03069816101787804, 0.028079891052061757, 0.025272213749531362, 0.022407885955512265, 0.019522257635110528, 0.01665408880415322, 0.013690779147167313, 0.010620568241640226, 0.007574513852244189, 0.004519836797465142, 0.0016154122973937235, -0.0010496218657825654, -0.003534326077921876, -0.005932700036112856, -0.008438826550096376, -0.011010848885321925, -0.013619460039667332, -0.016302186309091173, -0.018991841680044513, -0.021480319112092673, -0.023583855698521956, -0.025295336528072487, -0.026673338308047117, -0.027742150426563927, -0.028573958826771752, -0.0293116637695258, -0.029982556396270017, -0.030678556165621214, -0.03151435483288307, -0.03250998812497514, -0.03360432386983512, -0.03471095923759949, -0.035904136115868325, -0.03717785858878964, -0.03843829858396426, -0.039687098787839166, -0.04083574475139249, -0.04187812621042321, -0.04288452906994237, -0.04371824306255902, -0.04432065185892613, -0.04475451613188304, -0.04499107638233755, -0.04498519217796374, -0.04480005412861738, -0.04463761301462522, -0.044636424613608096, -0.04481314790899094, -0.045190977151739316, -0.045770429811020814, -0.0465362449408236, -0.047470993911089374, -0.04845248870186806, -0.04934877966677854, -0.050160555736446885, -0.050948768217588876, -0.05168573098488742, -0.052253058965728, -0.05264663266285433, -0.052913056181593074, -0.05306010933881184, -0.05320037226273451, -0.053347436611323476, -0.05340983148057044, -0.05336379091651196, -0.05324866321269645, -0.05304019420232895, -0.052772092763725, -0.05254381496038108, -0.05249508944374346, -0.052681475139016117, -0.05305490322084473, -0.05366543315881129, -0.05449625498067482, -0.05549393036526014, -0.05664981773619999, -0.05793476915494057, -0.05934213553536875, -0.06084393187152432, -0.062326096067383416, -0.06360541480401567, -0.06455276800059137, -0.06513982118372832, -0.06538157488488568, -0.0652970496046778, -0.0648738554123851, -0.0641159019553152, -0.06312873507458316, -0.06201998782279556, -0.0609591322818873, -0.06009750387840477, -0.05965336086861815, -0.059680074430943815, -0.060095180311443515, -0.060847078081302804, -0.06186314089673762, -0.06307103993078052, -0.0643361422439323, -0.06562874107129084, -0.06689786394993112, -0.06794151891742621, -0.06851087144652912, -0.06839544383541395, -0.06761361882673418, -0.06624603846918864, -0.06442580595182813, -0.06238959295638481, -0.060252025076980395, -0.05810929544647451, -0.056094464724476586, -0.05438280605595493, -0.05304703080403304, -0.05202833995213494, -0.05133764028187086, -0.05095253713254259, -0.0507928116270742, -0.05059821751783512, -0.05020422073173748, -0.04958180043168759, -0.04855055206625945, -0.046903987272304055, -0.04444749625221468, -0.04100610736092512, -0.03641203972335109, -0.030548323332449903, -0.02347849440541051, -0.01536848997344879, -0.006548877332304397, 0.0027596818573373152, 0.012558207662544858, 0.022893044668102065, 0.03383422054493489, 0.04530015609015242, 0.05727389055229744, 0.06974655515893602, 0.0827647720447356, 0.09623725825436409, 0.10997221488624011, 0.1239604361707943, 0.13814221249620534, 0.15254560114200785, 0.16718694321164823, 0.18213908890204009, 0.1975233961389233, 0.21338264023988696, 0.22968205893602603, 0.24617703779560504, 0.262553271594614, 0.2785498849784734, 0.2938562542950789, 0.30818438656770014, 0.3211769018155584, 0.33259504805528695, 0.3423151697885665, 0.349840989644172, 0.35455177014284533, 0.35608907750650626, 0.3542143347957578, 0.3486934675899937, 0.3395672422559585, 0.32694954828734357, 0.31124692008573673, 0.29307177666836287, 0.27294512224553197, 0.25171041296452845, 0.23006369754194428, 0.2085232527596863, 0.18756743299701967, 0.1675559847855022, 0.1486883295091558, 0.13102053065989652, 0.1146410768602259, 0.09955494422670595, 0.08560086307512767, 0.07254847371459171, 0.06009534126097053, 0.04791684128645689, 0.03571095468468001, 0.023179014209033778, 0.010354247203329408, -0.0024486522870886666, -0.015124388852892565, -0.027493783971036564, -0.039187088635083786, -0.04994605940286652, -0.05950151593008899, -0.0677056209119103, -0.07451588648583134, -0.07974513437851537, -0.08350605535156404, -0.08597051473158306, -0.0871578341647395, -0.08721383328252046, -0.08632468818083629, -0.08471632131818356, -0.08257027170944674, -0.08010913107245514, -0.07752868987024768, -0.07493403032229908, -0.07243044568794212, -0.07005916877058242, -0.06778824508871133, -0.06556284085744778, -0.0633607532279263, -0.06116238748185355, -0.05901849984036007, -0.056917306431272505, -0.05486135575339427, -0.052933623215989115, -0.051130133615265125, -0.049339891516551944, -0.047510249334377155, -0.045639708858732905, -0.04387209803409885, -0.04221786242144478, -0.040498436902832904, -0.038710338771826905, -0.03692247226736793, -0.035223409655891595, -0.033624886494154085, -0.03212061608283294, -0.030688024536204603, -0.029311377888299527, -0.028078643629828745, -0.02701968260978864, -0.026143393937491474, -0.02545858464398114, -0.024926885970891002, -0.02450244063394015, -0.02420423336132542, -0.024122157164215636, -0.02425780676452784, -0.02449150011228927, -0.024641679921172676, -0.024624755914248356, -0.024587598448723283, -0.024595632870551373, -0.024593862674752845, -0.024460736835579436, -0.024160174736743104, -0.02386062611242111, -0.023638639111190008, -0.023425493087514834, -0.023203297603189912, -0.023000446462746085, -0.022852619046674447, -0.022813803585211387, -0.02291007665064347, -0.02308902843416534, -0.023351294410119808, -0.0237260467441011, -0.02424718259967425, -0.024937071746096235, -0.0256948019857116, -0.026445218691865936, -0.027102308961568358, -0.02763040145609831, -0.02805577307000791, -0.028474520222466562, -0.028931691065671325, -0.0293121576189707, -0.029567947618745, -0.02968325744039051, -0.029655332020775596, -0.029449622228974723, -0.028948276887178815, -0.028223242270590494, -0.02748290215595743, -0.026775251053125934, -0.02609421213441969, -0.025486457039279695, -0.024957794531331696, -0.024543240188457095, -0.024273060689401008, -0.024098441377906746, -0.024036736702990853, -0.024001308718281978, -0.023903760529965557, -0.023827398302017924, -0.023810596468394854, -0.023836456705947542, -0.023812952999420753, -0.02365398790670591, -0.023329223421020798, -0.022820230707024866, -0.022194868213792646, -0.0214801337849508, -0.0206110586028198, -0.01956701875116186, -0.01849840056240969, -0.017646241545221444, -0.016974638748487356, -0.01642410555702069, -0.01601880863138709, -0.015790380375367327, -0.01577170139511293, -0.015925110233252954, -0.016221490680122175, -0.016579285566598226, -0.0169026160055958, -0.01701246527707938, -0.016819889465567242, -0.01638811789894733, -0.01574500069309652, -0.014843833908639125, -0.013642664520876773, -0.012202342928023484, -0.010656742307954452, -0.009057425482233445, -0.007463069966698781, -0.005924534868660253, -0.004430954957730011, -0.003128172596694889, -0.002141091621687674, -0.0014823115704595711, -0.0011345018178402662, -0.0011491347830231311, -0.0014267221724718502, -0.0018495902588247122, -0.002355771280324703, -0.002763101805875595, -0.002932369113468991, -0.002716612006261265, -0.0021311662723179246, -0.0013470049195411391, -0.0004786113589603403, 0.0003897890504305056, 0.0013308233426915894, 0.0024885598432078217, 0.0037697634603567455, 0.004985086353888345, 0.0060572476755521125, 0.007016105038765663, 0.007925018303354138, 0.008823513953024952, 0.009767254408519123, 0.010758005706733725, 0.011816765890628836, 0.012842396446436598, 0.013832543303847016, 0.014969336578386295, 0.016223408608202327, 0.017471395751548918, 0.01864275480412237, 0.01979828493200512, 0.020967424556253705, 0.022044296638522098, 0.02297247572116044, 0.023779168211230616, 0.02468021701887925, 0.025782577890907665, 0.027044929824294305, 0.02836042918869359, 0.029574389659662453, 0.030716528269291315, 0.03194539221771362, 0.03336990530363205, 0.03498789024895764, 0.036711183902569174, 0.03846672178406955, 0.040211615950451286, 0.041840409512169226, 0.04336610314633816, 0.044947388444011395, 0.046547063464411266, 0.04802434001157334, 0.049463437623074855, 0.050943448406325406, 0.052588691373767274, 0.054417280253550304, 0.056268924273746974, 0.05815472832311004, 0.06011337710179744, 0.06220844140775487, 0.06430645641481791, 0.06629267458324131, 0.06825649159025418, 0.07023563933656694, 0.07231152911035546, 0.07455820019919625, 0.07687386761070748, 0.07916904773151291, 0.08147322185707824, 0.08381383444673658, 0.08620246323595891, 0.08863999414620714, 0.09123242943758007, 0.09386859310956538, 0.09639599735859565, 0.09886826526449473, 0.10129899011500203, 0.1036490304378289, 0.10590955510669546, 0.10798956504399986, 0.10981049628366547, 0.11151060216233158, 0.11315323745686581, 0.11468611498801493, 0.11605841475085966, 0.11731796113917178, 0.11853336722623649, 0.11964915812687754, 0.12071247122165268, 0.12168254268006744, 0.12247379728287754, 0.12323635567682427, 0.12398386242873295, 0.12463661780160236, 0.12523476556556812, 0.12584041828211998, 0.12641911230973465, 0.1268519319907476, 0.1271491670741537, 0.12734362825436213, 0.1274184528489925, 0.12733034576920124, 0.12706915792916956, 0.12665034883743548, 0.1260715948055618, 0.12542478104585555, 0.12480574035091475, 0.12419729339722099, 0.12359633262166224, 0.1229495625235126, 0.1221948956657427, 0.12131656877231788, 0.1202767823814015, 0.11901011248811313, 0.11751567156087132, 0.11581416745625256, 0.11380333333341465, 0.11157002229121346, 0.1092478641453217, 0.10678896268662258, 0.1040744192326553, 0.10118460267374953, 0.09829675198352286, 0.09546642005592694, 0.09277284281472287, 0.09021704127289867, 0.0878573702896914, 0.0857756582075686, 0.08398518253114, 0.08251680131859601, 0.08126712742544757, 0.07994557019018801, 0.07849055250715713, 0.07688420691501024, 0.07507020654833167, 0.07299038687781588, 0.07045879735077784, 0.06755223675230135, 0.06443292320499609, 0.06119340364500609, 0.05793547486350919, 0.05475118932958667, 0.05164317167610941, 0.04867833922829771, 0.04599139215091733, 0.04372176198157811, 0.041871506332543364, 0.040333452305042136, 0.03916753891547401, 0.03828373725573712, 0.037680554816056464, 0.037331418447194266, 0.037017139890442426, 0.03662370933188131, 0.036102002482007935, 0.035452045134875994, 0.03473293381014728, 0.033832805740039974, 0.03266283893222263, 0.031269564316560784, 0.029783765188230067, 0.028511836733004288, 0.027511832972366156, 0.026681178281755848, 0.02606786879784246, 0.025659732460098707, 0.025389145701518352, 0.02527837998749905, 0.025221854393087363, 0.025092894065174128, 0.02491018793330069, 0.02465407710778764, 0.0242751231852958, 0.02369556163852863, 0.022919218138597687, 0.021835454548240214, 0.020360346424944227, 0.018676042468559924, 0.01693731230131188, 0.015128032846960604, 0.013270864827954344, 0.011457637371328551, 0.009837609403396903, 0.008535673725036557, 0.007610454772027853, 0.007034937306913103, 0.006692762567983102, 0.006514149070929516, 0.0065402734645921895, 0.006697979930434561, 0.0068807670513141355, 0.007113111792462662, 0.007300648818197953, 0.0074002592944861015, 0.0074366446321669, 0.007300987511521117, 0.007015366171830884, 0.006651056503066194, 0.0061258772355949126, 0.005486398882727317, 0.004867954481594111, 0.004389622593033082, 0.00406501048788347, 0.003818083929021171, 0.003749655462824692, 0.003868610475671752, 0.004149063770204719, 0.004492415537391497, 0.004859839032613838, 0.005258061585463691, 0.0056362684708671, 0.005975404191119921, 0.006254278984453498, 0.006423187719567741, 0.006423463026280945, 0.006251476872671779, 0.005912199183809703, 0.005485576971286799, 0.004974760238886604, 0.0043881948850783056, 0.003867953140327429, 0.003373933624166719, 0.0028354676783657936, 0.002440974486734309, 0.0022359459389813315, 0.002220042434117003, 0.002459763019273591, 0.002919697364395518, 0.003547405193023217, 0.004157067118087781, 0.004680800082138063, 0.005051584111998441, 0.005297498878893002, 0.005394672094658552, 0.005251806779953632, 0.004974673350575531, 0.004582889287315072, 0.004107829472677326, 0.0035779697822588873, 0.0029715251887970687, 0.0023379809177233083, 0.0016975912512784453, 0.0010558102284827554, 0.0005042955384364546, 0.00015696570496101967, -1.1004637589841372e-13]


plt.plot([i for i in range(len(initial_points))], initial_points)

input_for_model=initial_points+[0 for i in range(1499-len(initial_points))]+[801]

predicted_points = iterative_forecast(input_for_model, model, steps=10)
plt.show()